{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b27a10-191b-41a1-8a6c-a7de738030da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "✅ Full project scaffold successfully created in:\n",
      "D:\\Python_Projects\\Ultramafic_MLOPS\n",
      "The crucial __init__.py files have been added, and all Docker configurations are updated.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set the project root path\n",
    "PROJECT_ROOT = Path(r\"D:/Python_Projects/Ultramafic_MLOPS\").resolve()\n",
    "\n",
    "# --- Content Definitions (Using textwrap.dedent for clean code blocks) ---\n",
    "\n",
    "COMMON_SCRIPT_CONTENT = textwrap.dedent(\"\"\"\n",
    "    import yaml\n",
    "    from pathlib import Path\n",
    "\n",
    "    def read_params(config_path: Path) -> dict:\n",
    "        \\\"\\\"\\\"Reads the YAML configuration file and returns its content as a dictionary.\\\"\\\"\\\"\n",
    "        try:\n",
    "            with open(config_path, 'r') as yaml_file:\n",
    "                config = yaml.safe_load(yaml_file)\n",
    "            return config\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Configuration file not found at {config_path}\")\n",
    "            return {}\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(f\"Error parsing YAML file: {exc}\")\n",
    "            return {}\n",
    "\"\"\")\n",
    "\n",
    "DATA_INGESTION_CONTENT = textwrap.dedent(\"\"\"\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    from src.utils.common import read_params\n",
    "\n",
    "    def main():\n",
    "        CONFIG_PATH = Path(\"Config/params.yaml\")\n",
    "        params = read_params(CONFIG_PATH)\n",
    "        source_path = params['data_ingestion']['source_path']\n",
    "        output_file = params['data_ingestion']['output_file']\n",
    "\n",
    "        # Mock Data Creation for demonstration purposes if the source CSV is missing\n",
    "        if not Path(source_path).exists():\n",
    "            print(f\"Warning: Mocking data as source file not found at {source_path}.\")\n",
    "            data = pd.DataFrame({\n",
    "                'Tectonic setting': ['MOR', 'OIB', 'SSZ', 'MOR', 'OIB', 'SSZ'],\n",
    "                'SiO2': [45, 48, 51, 46, 49, 50],\n",
    "                'MgO': [8, 7, 6, 7.5, 6.5, 5.5],\n",
    "                'Al2O3': [15, 16, 17, 15.5, 16.5, 17.5],\n",
    "                'Col_1': range(6), 'Sample': ['A1', 'A2', 'A3', 'A4', 'A5', 'A6']\n",
    "            })\n",
    "        else:\n",
    "            try:\n",
    "                data = pd.read_csv(source_path)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: Source file not found at {source_path}. Please check config.\")\n",
    "                return\n",
    "\n",
    "        output_path = Path(output_file)\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        data.to_csv(output_path, index=False)\n",
    "        print(f\"Data Ingestion complete: {output_path}\")\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        main()\n",
    "\"\"\")\n",
    "\n",
    "DATA_PROCESSING_CONTENT = textwrap.dedent(\"\"\"\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from src.utils.common import read_params\n",
    "    import joblib\n",
    "    import numpy as np\n",
    "\n",
    "    def clr_transform(X):\n",
    "        # X assumed non-negative; add small offset for zeros\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        offset = 1e-9\n",
    "        X = X + offset\n",
    "        logX = np.log(X)\n",
    "        gm = np.mean(logX, axis=1, keepdims=True)\n",
    "        clr = logX - gm\n",
    "        return clr\n",
    "\n",
    "    def get_preprocessor(params, numerical_cols):\n",
    "        impute_strategy = params['data_processing']['impute_strategy']\n",
    "        numerical_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy=impute_strategy)),\n",
    "            ('clr', FunctionTransformer(clr_transform, validate=False)),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        return ColumnTransformer([('num', numerical_pipeline, numerical_cols)], remainder='drop')\n",
    "\n",
    "    def main():\n",
    "        CONFIG_PATH = Path(\"Config/params.yaml\")\n",
    "        params = read_params(CONFIG_PATH)\n",
    "        target_col = params['base']['target_column']\n",
    "        raw_data_path = Path(params['data_ingestion']['output_file'])\n",
    "        passthrough = params['data_processing']['passthrough_features']\n",
    "        pca_var = params['data_processing']['pca_variance_threshold']\n",
    "\n",
    "        processed_data_path = Path(\"data/processed/data_processed.pkl\")\n",
    "        pipeline_path = Path(\"models/preprocessing_pipeline.joblib\")\n",
    "\n",
    "        try:\n",
    "            data = pd.read_csv(raw_data_path)\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error: Raw data not found. Run data_ingestion first.\")\n",
    "            return\n",
    "\n",
    "        X = data.drop(columns=[target_col], errors='ignore')\n",
    "        y = data[target_col]\n",
    "\n",
    "        # Identify numerical columns (all except the passthrough feature 'Sample')\n",
    "        num_cols = [c for c in X.columns if X[c].dtype in ['int64', 'float64'] and c not in passthrough]\n",
    "\n",
    "        if not num_cols:\n",
    "            print(\"Error: No numerical columns found for preprocessing/PCA.\")\n",
    "            return\n",
    "\n",
    "        preprocessor = get_preprocessor(params, num_cols)\n",
    "\n",
    "        # Fit and transform the data using the scaler pipeline (which includes CLR)\n",
    "        X_trans = preprocessor.fit_transform(X)\n",
    "\n",
    "        # Convert scaled array back to DataFrame for PCA step\n",
    "        X_df = pd.DataFrame(X_trans, columns=num_cols)\n",
    "\n",
    "        # Apply PCA on the transformed numerical data (capturing pca_var variance)\n",
    "        pca = PCA(n_components=pca_var, random_state=params['data_ingestion']['random_state'])\n",
    "        pca_df = pd.DataFrame(pca.fit_transform(X_df), columns=[f'PCA_{i+1}' for i in range(pca.n_components_)])\n",
    "\n",
    "        # Concatenate PCA components, the passthrough column ('Sample'), and the target column\n",
    "        final = pd.concat([pca_df, X[passthrough].reset_index(drop=True), y.reset_index(drop=True)], axis=1)\n",
    "\n",
    "        final.to_pickle(processed_data_path)\n",
    "\n",
    "        # Save the complete pipeline (preprocessor + PCA)\n",
    "        joblib.dump(Pipeline([('pre', preprocessor), ('pca', pca)]), pipeline_path)\n",
    "        print(f\"Data processed and saved. PCA components found: {pca.n_components_}\")\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        main()\n",
    "\"\"\")\n",
    "\n",
    "MODEL_TRAINER_CONTENT = textwrap.dedent(\"\"\"\n",
    "    import joblib, optuna, pandas as pd\n",
    "    from pathlib import Path\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from xgboost import XGBClassifier\n",
    "    from src.utils.common import read_params\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "    def main():\n",
    "        CONFIG_PATH = Path(\"Config/params.yaml\")\n",
    "        params = read_params(CONFIG_PATH)\n",
    "        target_col = params['base']['target_column']\n",
    "\n",
    "        processed_data_path = Path(\"data/processed/data_processed.pkl\")\n",
    "        if not processed_data_path.exists():\n",
    "            print(f\"Error: Processed data not found at {processed_data_path}. Please run data_processing first.\")\n",
    "            return\n",
    "\n",
    "        data = pd.read_pickle(processed_data_path)\n",
    "\n",
    "        # Drop target and passthrough features (like 'Sample')\n",
    "        X = data.drop(columns=[target_col] + params['data_processing']['passthrough_features'])\n",
    "        y = data[target_col]\n",
    "\n",
    "        # --- Label Encoding for XGBoost ---\n",
    "        le = LabelEncoder()\n",
    "        y = pd.Series(le.fit_transform(y), name=target_col)\n",
    "        print(f\"Target labels encoded: {le.classes_} -> {le.transform(le.classes_)}\")\n",
    "        # -------------------------------------\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,\n",
    "            test_size=params['data_ingestion']['test_size'],\n",
    "            random_state=params['data_ingestion']['random_state'],\n",
    "            stratify=y\n",
    "        )\n",
    "\n",
    "        xgb_default = params['model_trainer']['xgb_default']\n",
    "\n",
    "        def objective(trial):\n",
    "            optuna_params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 800),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            }\n",
    "            full_params = {**optuna_params, **xgb_default}\n",
    "\n",
    "            m = XGBClassifier(use_label_encoder=False, **full_params)\n",
    "            m.fit(X_train, y_train)\n",
    "            return m.score(X_test, y_test)\n",
    "\n",
    "        try:\n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(objective, n_trials=params['model_trainer']['n_trials'])\n",
    "        except Exception as e:\n",
    "            print(f\"Optuna failed during optimization: {e}\")\n",
    "            study = type('', (object,), {'best_params': {}})()\n",
    "            study.best_params = params['model_trainer']['xgb_default']\n",
    "            print(\"Falling back to default model parameters due to Optuna failure.\")\n",
    "\n",
    "        print(\"Best parameters found:\", study.best_params)\n",
    "\n",
    "        final_model_params = {**study.best_params, **xgb_default}\n",
    "        model = XGBClassifier(use_label_encoder=False, **final_model_params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        Path(\"models\").mkdir(exist_ok=True)\n",
    "        preproc_path = Path(\"models/preprocessing_pipeline.joblib\")\n",
    "        preprocessor = None\n",
    "        if preproc_path.exists():\n",
    "            try:\n",
    "                preprocessor = joblib.load(preproc_path)\n",
    "                print(f\"Loaded preprocessor from {preproc_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: could not load preprocessor: {e}\")\n",
    "                preprocessor = None\n",
    "        else:\n",
    "            print(\"Warning: preprocessing_pipeline.joblib not found. Ensure data_processing.py was run.\")\n",
    "\n",
    "        # Save combined pipeline (preprocessor -> classifier) for raw-input inference\n",
    "        if preprocessor is not None:\n",
    "            full_pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", model)])\n",
    "        else:\n",
    "            full_pipeline = Pipeline([(\"classifier\", model)])\n",
    "            print(\"Saved pipeline contains only the classifier because preprocessor was not found.\")\n",
    "\n",
    "        # Save both formats for compatibility\n",
    "        joblib.dump({\"pipeline\": full_pipeline, \"label_encoder\": le}, \"models/final_pipeline.joblib\")\n",
    "        joblib.dump({'model': model, 'label_encoder': le}, \"models/best_model_tuned.joblib\")\n",
    "        print(\"Saved models/final_pipeline.joblib (pipeline + label_encoder) and models/best_model_tuned.joblib (compat).\")\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        main()\n",
    "\"\"\")\n",
    "\n",
    "MODEL_EVALUATION_CONTENT = textwrap.dedent(\"\"\"\n",
    "    import pandas as pd, joblib, json\n",
    "    from pathlib import Path\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "    import seaborn as sns, matplotlib.pyplot as plt\n",
    "    from src.utils.common import read_params\n",
    "\n",
    "    def main():\n",
    "        CONFIG_PATH = Path(\"Config/params.yaml\")\n",
    "        params = read_params(CONFIG_PATH)\n",
    "        ycol = params['base']['target_column']\n",
    "        df = pd.read_pickle(\"data/processed/data_processed.pkl\")\n",
    "        X = df.drop(columns=[ycol] + params['data_processing']['passthrough_features'])\n",
    "        y = df[ycol]\n",
    "\n",
    "        try:\n",
    "            model_and_le = joblib.load(\"models/best_model_tuned.joblib\")\n",
    "            le = model_and_le['label_encoder']\n",
    "        except Exception:\n",
    "            print(\"Error: Could not load model or LabelEncoder. Ensure model_trainer ran successfully.\")\n",
    "            return\n",
    "\n",
    "        y_encoded = le.transform(y)\n",
    "\n",
    "        X_train, X_test, y_train_orig, y_test_orig = train_test_split(X, y_encoded, test_size=params['data_ingestion']['test_size'],\n",
    "                                                                      random_state=params['data_ingestion']['random_state'], stratify=y_encoded)\n",
    "\n",
    "        model = model_and_le['model']\n",
    "        y_pred_encoded = model.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test_orig, y_pred_encoded)\n",
    "        p, r, f1, _ = precision_recall_fscore_support(y_test_orig, y_pred_encoded, average='macro')\n",
    "        res = dict(accuracy=acc, precision=p, recall=r, f1=f1)\n",
    "\n",
    "        Path(\"metrics\").mkdir(exist_ok=True)\n",
    "        with open(\"metrics/final_metrics.json\", \"w\") as f: json.dump(res, f, indent=2)\n",
    "\n",
    "        # Confusion Matrix using original class names (le.classes_)\n",
    "        cm = confusion_matrix(y_test_orig, y_pred_encoded)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                    xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.savefig(\"metrics/confusion_matrix.png\")\n",
    "        plt.close()\n",
    "        print(\"Evaluation complete.\")\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        main()\n",
    "\"\"\")\n",
    "\n",
    "API_CONTENT = textwrap.dedent(\"\"\"\n",
    "    from fastapi import FastAPI, UploadFile, File, HTTPException\n",
    "    from pydantic import BaseModel\n",
    "    import pandas as pd, joblib, os, io\n",
    "    from pathlib import Path\n",
    "    from src.utils.common import read_params\n",
    "\n",
    "    app = FastAPI(title=\"Ultramafic ML API\", version=\"0.1\")\n",
    "    CONFIG_PATH = Path(\"Config/params.yaml\")\n",
    "    params = read_params(CONFIG_PATH)\n",
    "    MODEL_PATH = params.get(\"api\", {}).get(\"model_path\", \"models/final_pipeline.joblib\")\n",
    "\n",
    "    def load_model_artifact(path):\n",
    "        if not Path(path).exists():\n",
    "            raise FileNotFoundError(f\"Model artifact not found at {path}\")\n",
    "        obj = joblib.load(path)\n",
    "        if isinstance(obj, dict):\n",
    "            if 'pipeline' in obj:\n",
    "                pipeline = obj['pipeline']\n",
    "                le = obj.get('label_encoder', None)\n",
    "                return pipeline, le\n",
    "            elif 'model' in obj:\n",
    "                model = obj['model']\n",
    "                le = obj.get('label_encoder', None)\n",
    "                return model, le\n",
    "        if hasattr(obj, \"predict\"):\n",
    "            return obj, None\n",
    "        raise ValueError(\"Unrecognized model artifact format.\")\n",
    "\n",
    "    # Load model and LabelEncoder once at startup\n",
    "    try:\n",
    "        model, le = load_model_artifact(MODEL_PATH)\n",
    "    except Exception as e:\n",
    "        model = None\n",
    "        le = None\n",
    "        print(f\"FATAL: Could not load model artifact: {e}\")\n",
    "\n",
    "    @app.get(\"/health\")\n",
    "    def health():\n",
    "        return {\"status\": \"ok\", \"model_exists\": bool(model)}\n",
    "\n",
    "    class PredictReq(BaseModel):\n",
    "        records: list\n",
    "\n",
    "    @app.post(\"/predict\")\n",
    "    def predict(payload: PredictReq):\n",
    "        if not model:\n",
    "            raise HTTPException(status_code=503, detail=\"Model is not available or trained yet.\")\n",
    "\n",
    "        df = pd.DataFrame(payload.records)\n",
    "        try:\n",
    "            preds_encoded = model.predict(df)\n",
    "            preds_decoded = le.inverse_transform(preds_encoded).tolist() if le is not None else preds_encoded.tolist()\n",
    "            probs = model.predict_proba(df).tolist() if hasattr(model, \"predict_proba\") else None\n",
    "            return {\"predictions\": preds_decoded, \"probabilities\": probs}\n",
    "        except Exception as e:\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "    @app.post(\"/predict_csv\")\n",
    "    def predict_csv(file: UploadFile = File(...)):\n",
    "        if not model:\n",
    "            raise HTTPException(status_code=503, detail=\"Model is not available or trained yet.\")\n",
    "\n",
    "        content = file.file.read()\n",
    "        df = pd.read_csv(io.BytesIO(content))\n",
    "        try:\n",
    "            preds_encoded = model.predict(df)\n",
    "            preds_decoded = le.inverse_transform(preds_encoded).tolist() if le is not None else preds_encoded.tolist()\n",
    "            return {\"n_rows\": len(df), \"predictions\": preds_decoded}\n",
    "        except Exception as e:\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "\"\"\")\n",
    "\n",
    "STREAMLIT_UI_CONTENT = textwrap.dedent(\"\"\"\n",
    "    import streamlit as st, pandas as pd, joblib, os, io\n",
    "    from pathlib import Path\n",
    "    from src.utils.common import read_params\n",
    "    import numpy as np\n",
    "\n",
    "    # Set page configuration must be at the very top\n",
    "    st.set_page_config(page_title=\"Ultramafic ML UI\", layout=\"wide\")\n",
    "\n",
    "    CONFIG_PATH = Path(\"Config/params.yaml\")\n",
    "    params = read_params(CONFIG_PATH)\n",
    "    MODEL_PATH = params.get(\"api\", {}).get(\"model_path\", \"models/final_pipeline.joblib\")\n",
    "\n",
    "    st.title(\"Ultramafic ML - Streamlit UI\")\n",
    "\n",
    "    pipeline = None\n",
    "    label_encoder = None\n",
    "    loaded_from = None\n",
    "\n",
    "    # Load combined pipeline (preferred) or fallback\n",
    "    PIPE_PATHS = [Path(\"models/final_pipeline.joblib\"), Path(\"models/best_model_tuned.joblib\"), Path(\"models/pipeline.joblib\")]\n",
    "    for p in PIPE_PATHS:\n",
    "        if p.exists():\n",
    "            try:\n",
    "                obj = joblib.load(p)\n",
    "                if isinstance(obj, dict):\n",
    "                    if 'pipeline' in obj:\n",
    "                        pipeline = obj['pipeline']\n",
    "                        label_encoder = obj.get('label_encoder', None)\n",
    "                    elif 'model' in obj:\n",
    "                        if hasattr(obj['model'], \"named_steps\"):\n",
    "                            pipeline = obj['model']\n",
    "                        else:\n",
    "                            pipeline = obj['model']\n",
    "                        label_encoder = obj.get('label_encoder', None)\n",
    "                else:\n",
    "                    if hasattr(obj, \"predict\"):\n",
    "                        pipeline = obj\n",
    "                loaded_from = p\n",
    "                break\n",
    "            except Exception as e:\n",
    "                st.sidebar.error(f\"Failed to load {p}: {e}\")\n",
    "\n",
    "    if pipeline is None:\n",
    "        st.sidebar.warning(\"No pipeline detected. Please run training to create models/final_pipeline.joblib.\")\n",
    "    else:\n",
    "        st.sidebar.success(f\"Loaded pipeline from {loaded_from}\")\n",
    "\n",
    "    def read_uploaded_file(uploaded_file, file_path_hint):\n",
    "        file_extension = Path(file_path_hint).suffix.lower()\n",
    "        content = uploaded_file.getvalue()\n",
    "        if file_extension == '.csv':\n",
    "            st.info(f\"Reading {uploaded_file.name} as CSV...\")\n",
    "            return pd.read_csv(io.StringIO(content.decode('utf-8')))\n",
    "        elif file_extension in ['.xlsx', '.xls']:\n",
    "            st.info(f\"Reading {uploaded_file.name} as Excel...\")\n",
    "            return pd.read_excel(io.BytesIO(content))\n",
    "        else:\n",
    "            st.warning(f\"Skipping unsupported file type: {uploaded_file.name}\")\n",
    "            return None\n",
    "\n",
    "    col1, col2 = st.columns([2, 1])\n",
    "\n",
    "    with col1:\n",
    "        uploaded_files = st.file_uploader(\"Upload CSV / Excel files (multi allowed)\", type=[\"csv\", \"xlsx\", \"xls\"], accept_multiple_files=True)\n",
    "        if uploaded_files:\n",
    "            if st.button(\"Predict Uploaded Files\"):\n",
    "                if pipeline is None:\n",
    "                    st.error(\"No pipeline available. Train and save pipeline to models/final_pipeline.joblib.\")\n",
    "                else:\n",
    "                    for uploaded_file in uploaded_files:\n",
    "                        st.markdown(f\"**File:** {uploaded_file.name}\")\n",
    "                        try:\n",
    "                            suffix = Path(uploaded_file.name).suffix.lower()\n",
    "                            if suffix in [\".xlsx\", \".xls\"]:\n",
    "                                df = pd.read_excel(uploaded_file)\n",
    "                            else:\n",
    "                                df = pd.read_csv(uploaded_file)\n",
    "                        except Exception as e:\n",
    "                            st.error(f\"Failed to read {uploaded_file.name}: {e}\")\n",
    "                            continue\n",
    "\n",
    "                        # Drop passthrough columns if present\n",
    "                        features_df = df.drop(columns=params['data_processing']['passthrough_features'], errors='ignore')\n",
    "                        try:\n",
    "                            preds = pipeline.predict(features_df)\n",
    "                            probs = pipeline.predict_proba(features_df) if hasattr(pipeline, \"predict_proba\") else None\n",
    "                            if label_encoder is not None:\n",
    "                                preds_decoded = label_encoder.inverse_transform(preds)\n",
    "                                class_names = list(label_encoder.classes_)\n",
    "                            else:\n",
    "                                preds_decoded = preds\n",
    "                                class_names = list(pipeline.classes_) if hasattr(pipeline, \"classes_\") else []\n",
    "                            out_df = df.copy().reset_index(drop=True)\n",
    "                            out_df[\"Predicted\"] = preds_decoded\n",
    "                            if probs is not None:\n",
    "                                probs_df = pd.DataFrame(probs, columns=class_names)\n",
    "                                out_df = pd.concat([out_df, probs_df.reset_index(drop=True)], axis=1)\n",
    "                                out_df[\"Confidence\"] = probs.max(axis=1)\n",
    "                            st.dataframe(out_df.head(20))\n",
    "                            csv = out_df.to_csv(index=False).encode(\"utf-8\")\n",
    "                            st.download_button(f\"Download predictions for {uploaded_file.name}\", data=csv, file_name=f\"predictions_{uploaded_file.name}.csv\")\n",
    "                            st.success(\"Prediction completed.\")\n",
    "                        except Exception as e:\n",
    "                            st.error(f\"Prediction failed for {uploaded_file.name}: {e}\")\n",
    "                            # diagnostics\n",
    "                            if hasattr(pipeline, \"feature_names_in_\"):\n",
    "                                expected = set(pipeline.feature_names_in_)\n",
    "                                provided = set(features_df.columns)\n",
    "                                missing = expected - provided\n",
    "                                if missing:\n",
    "                                    st.info(f\"Missing input fields expected by model: {sorted(list(missing))}\")\n",
    "                            else:\n",
    "                                st.info(\"Ensure uploaded file columns match training raw feature names.\")\n",
    "\n",
    "    with col2:\n",
    "        st.subheader(\"Single sample input\")\n",
    "        st.markdown(\"Enter one sample manually. Use key=value per line or a single CSV line.\")\n",
    "        manual_mode = st.radio(\"Input mode:\", [\"Key=Value lines\", \"Single-line CSV\"])\n",
    "        sample_input = st.text_area(\"Paste sample here\", height=200, placeholder=\"e.g.\\nSiO2=45\\nAl2O3=15\\nTiO2=0.5\\n...\")\n",
    "\n",
    "        if st.button(\"Predict single sample\"):\n",
    "            if pipeline is None:\n",
    "                st.error(\"No pipeline available. Train and save pipeline first.\")\n",
    "            else:\n",
    "                sample_df = None\n",
    "                try:\n",
    "                    if manual_mode == \"Key=Value lines\":\n",
    "                        lines = [L.strip() for L in sample_input.splitlines() if L.strip()]\n",
    "                        d = {}\n",
    "                        for L in lines:\n",
    "                            if \"=\" in L:\n",
    "                                k, v = L.split(\"=\", 1)\n",
    "                                d[k.strip()] = float(v.strip())\n",
    "                            else:\n",
    "                                raise ValueError(\"Invalid line format. Expect key=value per line.\")\n",
    "                        sample_df = pd.DataFrame([d])\n",
    "                    else:\n",
    "                        txt = sample_input.strip()\n",
    "                        if \",\" in txt:\n",
    "                            try:\n",
    "                                sample_df = pd.read_csv(io.StringIO(txt))\n",
    "                            except Exception:\n",
    "                                vals = [v.strip() for v in txt.split(\",\")]\n",
    "                                expected = None\n",
    "                                if hasattr(pipeline, \"feature_names_in_\"):\n",
    "                                    expected = list(pipeline.feature_names_in_)\n",
    "                                if expected and len(vals) == len(expected):\n",
    "                                    sample_df = pd.DataFrame([dict(zip(expected, map(float, vals)))])\n",
    "                                else:\n",
    "                                    raise ValueError(\"CSV line does not match expected columns. Provide header or key=value lines.\")\n",
    "                        else:\n",
    "                            raise ValueError(\"CSV mode expects comma-separated values.\")\n",
    "                except Exception as e:\n",
    "                    st.error(f\"Could not parse sample input: {e}\")\n",
    "                    st.info(\"Examples:\\nSiO2=45\\nAl2O3=15\\n...  OR  SiO2,Al2O3,TiO2\\\\n45,15,0.5\")\n",
    "                    sample_df = None\n",
    "\n",
    "                if sample_df is not None:\n",
    "                    try:\n",
    "                        preds = pipeline.predict(sample_df)\n",
    "                        probs = pipeline.predict_proba(sample_df) if hasattr(pipeline, \"predict_proba\") else None\n",
    "                        if label_encoder is not None:\n",
    "                            pred_label = label_encoder.inverse_transform(preds)[0]\n",
    "                            class_names = list(label_encoder.classes_)\n",
    "                        else:\n",
    "                            pred_label = preds[0]\n",
    "                            class_names = list(pipeline.classes_) if hasattr(pipeline, \"classes_\") else []\n",
    "                        st.write(\"**Predicted class:**\", pred_label)\n",
    "                        if probs is not None:\n",
    "                            prob_series = pd.Series(probs[0], index=class_names)\n",
    "                            st.table(prob_series.sort_values(ascending=False).to_frame(\"Probability\"))\n",
    "                            st.info(f\"Confidence (max probability): {prob_series.max():.3f}\")\n",
    "                        else:\n",
    "                            st.info(\"Model did not provide probability estimates.\")\n",
    "                    except Exception as e:\n",
    "                        st.error(f\"Prediction failed: {e}\")\n",
    "                        st.warning(\"Ensure your input fields match training raw feature names and types.\")\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    # show expected columns if available\n",
    "    expected_cols = None\n",
    "    if hasattr(pipeline, \"feature_names_in_\"):\n",
    "        expected_cols = list(pipeline.feature_names_in_)\n",
    "    if expected_cols:\n",
    "        st.write(\"**Expected raw input columns (example):**\")\n",
    "        st.write(\", \".join(expected_cols))\n",
    "    else:\n",
    "        st.write(\"Could not infer expected raw input column names automatically. Ensure your input includes the same raw oxide/REE column names used during training.\")\n",
    "\"\"\")\n",
    "\n",
    "PARAMS_YAML_CONTENT = textwrap.dedent(\"\"\"\n",
    "    base:\n",
    "      target_column: \"Tectonic setting\"\n",
    "    data_ingestion:\n",
    "      # NOTE: Update this path to your actual CSV file location.\n",
    "      source_path: \"D:/Python_Projects/AI-ML - Copy.csv\"\n",
    "      output_file: \"data/raw/ingested.csv\"\n",
    "      test_size: 0.2\n",
    "      random_state: 42\n",
    "    data_processing:\n",
    "      passthrough_features:\n",
    "        - \"Sample\"\n",
    "      impute_strategy: \"median\"\n",
    "      pca_variance_threshold: 0.95\n",
    "    model_trainer:\n",
    "      n_trials: 20\n",
    "      xgb_default:\n",
    "        eval_metric: \"mlogloss\"\n",
    "    model_evaluation:\n",
    "      min_importance_threshold: 0.01\n",
    "      confusion_matrix_path: \"reports/confusion_matrix.png\"\n",
    "      final_metrics_path: \"metrics/final_metrics.json\"\n",
    "    api:\n",
    "      model_path: \"models/final_pipeline.joblib\"\n",
    "    ui:\n",
    "      default_sample_csv: \"data/raw/ingested.csv\"\n",
    "\"\"\")\n",
    "\n",
    "REQUIREMENTS_CONTENT = textwrap.dedent(\"\"\"\n",
    "    fastapi\n",
    "    uvicorn[standard]\n",
    "    streamlit\n",
    "    scikit-learn\n",
    "    xgboost\n",
    "    pandas\n",
    "    numpy\n",
    "    optuna\n",
    "    joblib\n",
    "    shap\n",
    "    matplotlib\n",
    "    seaborn\n",
    "    pyyaml\n",
    "    openpyxl\n",
    "    python-multipart\n",
    "\"\"\")\n",
    "\n",
    "DOCKERFILE_STREAMLIT_CONTENT = textwrap.dedent(\"\"\"\n",
    "    FROM python:3.11-slim\n",
    "    WORKDIR /app\n",
    "    COPY requirements.txt .\n",
    "    RUN pip install --no-cache-dir -r requirements.txt\n",
    "    COPY . .\n",
    "    EXPOSE 8501\n",
    "    CMD [\"streamlit\",\"run\",\"src/ui_app.py\",\"--server.port\",\"8501\",\"--server.address\",\"0.0.0.0\",\"--server.maxUploadSize\",\"200\"]\n",
    "\"\"\")\n",
    "\n",
    "DOCKERFILE_FASTAPI_CONTENT = textwrap.dedent(\"\"\"\n",
    "    FROM python:3.11-slim\n",
    "    WORKDIR /app\n",
    "    COPY requirements.txt .\n",
    "    RUN pip install --no-cache-dir -r requirements.txt\n",
    "    COPY . .\n",
    "    EXPOSE 8000\n",
    "    CMD [\"uvicorn\",\"src.api:app\",\"--host\",\"0.0.0.0\",\"--port\",\"8000\"]\n",
    "\"\"\")\n",
    "\n",
    "DOCKER_COMPOSE_CONTENT = textwrap.dedent(\"\"\"\n",
    "    version: '3.8'\n",
    "\n",
    "    services:\n",
    "      api:\n",
    "        build:\n",
    "          context: .\n",
    "          dockerfile: Dockerfile.fastapi\n",
    "        container_name: ultramafic_mlops-api\n",
    "        restart: unless-stopped\n",
    "        ports:\n",
    "          - \"8000:8000\"\n",
    "        volumes:\n",
    "          - ./models:/app/models:ro         \n",
    "\n",
    "      ui:\n",
    "        build:\n",
    "          context: .\n",
    "          dockerfile: Dockerfile.streamlit\n",
    "        container_name: ultramafic_mlops-ui\n",
    "        restart: unless-stopped\n",
    "        volumes:\n",
    "          - ./models:/app/models:ro        \n",
    "          - ./src:/app/src:rw              \n",
    "          - ./data:/app/data:rw            \n",
    "          - ./logs/ui:/app/logs:rw         \n",
    "        ports:\n",
    "          - \"8501:8501\"\n",
    "        environment:\n",
    "          - STREAMLIT_SERVER_PORT=8501\n",
    "          - STREAMLIT_SERVER_HEADLESS=true\n",
    "          - STREAMLIT_SERVER_ADDRESS=0.0.0.0\n",
    "          - PYTHONUNBUFFERED=1\n",
    "          - TZ=Asia/Kolkata\n",
    "        healthcheck:\n",
    "          test: [\"CMD-SHELL\", \"curl -fsS http://localhost:8501/ || exit 1\"]\n",
    "          interval: 15s\n",
    "          timeout: 5s\n",
    "          retries: 5\n",
    "\"\"\")\n",
    "\n",
    "DOCKERIGNORE_CONTENT = textwrap.dedent(\"\"\"\n",
    "    __pycache__\n",
    "    *.pyc\n",
    "    venv/\n",
    "    data/\n",
    "    *.ipynb\n",
    "    .git\n",
    "\"\"\")\n",
    "\n",
    "README_CONTENT = textwrap.dedent(\"\"\"\n",
    "    # Ultramafic_MLOPS\n",
    "\n",
    "    Project scaffold created by setup_full_project.py.\n",
    "\n",
    "    Run pipeline, then build and start API + UI with Docker Compose:\n",
    "\n",
    "    ```bash\n",
    "    python src/data_ingestion.py\n",
    "    python src/data_processing.py\n",
    "    python src/model_trainer.py\n",
    "    python src/model_evaluation.py\n",
    "    docker compose up --build\n",
    "    ```\n",
    "\n",
    "    Services:\n",
    "    - FastAPI: http://localhost:8000/docs\n",
    "    - Streamlit: http://localhost:8501\n",
    "\"\"\")\n",
    "\n",
    "# --- Execution Logic ---\n",
    "\n",
    "def write_file(path: Path, content: str):\n",
    "    \"\"\"Writes content to a file, stripping surrounding whitespace.\"\"\"\n",
    "    path.write_text(content.strip(), encoding=\"utf8\")\n",
    "\n",
    "def create_files():\n",
    "    # 1. Ensure directories exist\n",
    "    os.makedirs(PROJECT_ROOT / \"src\" / \"utils\", exist_ok=True)\n",
    "    os.makedirs(PROJECT_ROOT / \"Config\", exist_ok=True)\n",
    "    os.makedirs(PROJECT_ROOT / \"data\" / \"raw\", exist_ok=True)\n",
    "    os.makedirs(PROJECT_ROOT / \"data\" / \"processed\", exist_ok=True)\n",
    "    os.makedirs(PROJECT_ROOT / \"models\", exist_ok=True)\n",
    "    os.makedirs(PROJECT_ROOT / \"metrics\", exist_ok=True)\n",
    "    os.makedirs(PROJECT_ROOT / \"logs\" / \"ui\", exist_ok=True)\n",
    "\n",
    "    # 2. CRUCIAL FIX: Make src and src/utils Python packages\n",
    "    write_file(PROJECT_ROOT / \"src\" / \"__init__.py\", \"\")\n",
    "    write_file(PROJECT_ROOT / \"src\" / \"utils\" / \"__init__.py\", \"\")\n",
    "\n",
    "    # 3. Write all file contents\n",
    "    files_to_create = {\n",
    "        PROJECT_ROOT / \"src\" / \"utils\" / \"common.py\": COMMON_SCRIPT_CONTENT,\n",
    "        PROJECT_ROOT / \"src\" / \"data_ingestion.py\": DATA_INGESTION_CONTENT,\n",
    "        PROJECT_ROOT / \"src\" / \"data_processing.py\": DATA_PROCESSING_CONTENT,\n",
    "        PROJECT_ROOT / \"src\" / \"model_trainer.py\": MODEL_TRAINER_CONTENT,\n",
    "        PROJECT_ROOT / \"src\" / \"model_evaluation.py\": MODEL_EVALUATION_CONTENT,\n",
    "        PROJECT_ROOT / \"src\" / \"api.py\": API_CONTENT,\n",
    "        PROJECT_ROOT / \"src\" / \"ui_app.py\": STREAMLIT_UI_CONTENT,\n",
    "        PROJECT_ROOT / \"Config\" / \"params.yaml\": PARAMS_YAML_CONTENT,\n",
    "        PROJECT_ROOT / \"requirements.txt\": REQUIREMENTS_CONTENT,\n",
    "        PROJECT_ROOT / \"Dockerfile.streamlit\": DOCKERFILE_STREAMLIT_CONTENT,\n",
    "        PROJECT_ROOT / \"Dockerfile.fastapi\": DOCKERFILE_FASTAPI_CONTENT,\n",
    "        PROJECT_ROOT / \"docker-compose.yml\": DOCKER_COMPOSE_CONTENT,\n",
    "        PROJECT_ROOT / \".dockerignore\": DOCKERIGNORE_CONTENT,\n",
    "        PROJECT_ROOT / \"README.md\": README_CONTENT,\n",
    "    }\n",
    "\n",
    "    for path, content in files_to_create.items():\n",
    "        write_file(path, content)\n",
    "\n",
    "    print(\"---\")\n",
    "    print(\"✅ Full project scaffold successfully created in:\")\n",
    "    print(f\"{PROJECT_ROOT}\")\n",
    "    print(\"The crucial __init__.py files have been added, and all Docker configurations are updated.\")\n",
    "    print(\"---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        create_files()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ FATAL EXECUTION ERROR: {e}\")\n",
    "        print(\"Please ensure you are running this script directly from the command line.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23544b47-9bb8-44c7-bb4d-5fb808feebdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
